{
  "hash": "d900033f130beb4b054bb38f681a8aa5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Recipe in tidymodels\"\n\nauthor: \"Tony Duan\"\n\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: false\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n# 1. What is a Recipe?\n\nThe `recipes` package, part of the `tidymodels` ecosystem, provides a powerful and flexible framework for data preprocessing. A **recipe** is an object that defines a series of steps for transforming your data before it is used to train a model. Think of it as a blueprint for data preparation.\n\nUsing `recipes` is beneficial because it:\n\n-   **Separates preprocessing from modeling:** Keeps your code clean and organized.\n-   **Prevents data leakage:** Ensures that information from the test set does not \"leak\" into the training process, which is a common and subtle bug in machine learning. For example, when normalizing data, the mean and standard deviation should be calculated from the training data only and then applied to both the training and test sets.\n-   **Is highly extensible:** Provides a wide range of built-in steps for common tasks and allows you to create your own custom steps.\n\n# 2. Creating a Sample Dataset\n\nLet's create a small, messy dataset to demonstrate how recipes work. It contains a mix of data types, missing values (`NA`), and categorical variables with many levels.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\nlibrary(tidyverse)\n\n# Create a sample training dataset\ntrees_train <- tibble(\n  tree_id = 1:10,\n  species = factor(c(\"Oak\", \"Pine\", \"Maple\", \"Oak\", \"Pine\", \"Hickory\", \"Birch\", \"Oak\", \"Pine\", \"Maple\")),\n  date_planted = as.Date(c(\"2001-01-10\", \"2005-03-15\", \"2002-07-20\", \"2010-11-01\", \"2008-09-12\", \n                           \"2001-04-25\", \"2019-06-30\", \"2004-02-18\", \"2006-08-05\", \"2011-12-21\")),\n  caretaker = c(\"City\", \"Private\", \"City\", \"City\", \"Private\", \"Non-Profit\", \"City\", \"Private\", \"City\", \"Private\"),\n  site_info = c(\"Park A\", \"Yard B\", \"Park C\", \"Park A\", \"Yard D\", \"Reserve E\", \"Park F\", \"Yard G\", \"Park A\", \"Yard H\"),\n  height_m = c(10, 12, 8, 15, 13, 11, 2, 14, 12, 9),\n  diameter_cm = c(30, 35, 25, 40, 38, 32, 5, 39, 36, NA), # Added a missing value\n  legal_status = factor(c(\"Protected\", \"Not Protected\", \"Protected\", \"Protected\", \"Not Protected\", \n                          \"Protected\", \"Not Protected\", \"Protected\", \"Not Protected\", \"Protected\"))\n)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sample test set (new data)\nnew_trees_data <- tibble(\n  tree_id = 11:12,\n  species = factor(c(\"Oak\", \"Redwood\")),\n  date_planted = as.Date(c(\"2015-05-10\", \"2021-11-15\")),\n  caretaker = c(\"City\", \"State\"),\n  site_info = c(\"Park A\", \"National Park I\"),\n  height_m = c(5, 1),\n  diameter_cm = c(15, 2),\n  legal_status = factor(c(\"Protected\", \"Protected\"))\n)\n```\n:::\n\n# 3. Defining a Recipe\n\nA recipe is created with the `recipe()` function. You specify the model formula (`legal_status ~ .` means we want to predict `legal_status` using all other variables) and the training data.\n\nThen, you pipe (`%>%`) this object into a series of `step_*()` functions, each performing a specific preprocessing task.\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_rec <- recipe(legal_status ~ ., data = trees_train) %>%\n  \n  # 1. Assign roles: `update_role()` tells the recipe that `tree_id` is just an identifier\n  # and should not be used as a predictor or outcome. This prevents it from being included\n  # in the model matrix but keeps it in the data for reference.\n  update_role(tree_id, new_role = \"ID\") %>%\n  \n  # 2. Impute missing values: `step_impute_mean` will fill in the NA in `diameter_cm`\n  # with the mean of the non-missing values from the training set.\n  step_impute_mean(all_numeric_predictors()) %>%\n  \n  # 3. Collapse infrequent categories: `step_other()` is great for high-cardinality\n  # categorical variables. It pools infrequent levels into a single \"other\" category.\n  step_other(species, caretaker, site_info, threshold = 0.2, other = \"other_level\") %>%\n  \n  # 4. Create date features: Instead of using the exact date, which might not be a good\n  # predictor, we can extract the year. `step_rm()` then removes the original date column.\n  step_date(date_planted, features = c(\"year\")) %>%\n  step_rm(date_planted) %>%\n  \n  # 5. Create dummy variables: `step_dummy()` converts all nominal (categorical) predictors\n  # into numeric binary (0/1) columns. This is required for many modeling algorithms.\n  step_dummy(all_nominal_predictors()) %>%\n  \n  # 6. Remove zero-variance predictors: `step_zv()` removes any predictor columns that\n  # contain only a single unique value, as these have no predictive power.\n  step_zv(all_predictors()) %>% \n  \n  # 7. Normalize numeric predictors: `step_normalize()` scales all numeric predictors to have\n  # a mean of 0 and a standard deviation of 1. This helps many models converge faster.\n  step_normalize(all_numeric_predictors())\n\n# Print the recipe to see the defined steps\ntree_rec\n```\n:::\n\n# 4. Prepping and Baking the Recipe\n\nDefining the recipe doesn't actually perform the transformations. This is a crucial concept.\n\n1.  **`prep(recipe)`:** You first need to **`prep()`** the recipe. This function estimates the required parameters from the **training data only**. For example, it learns the means and standard deviations for normalization, the levels to collapse for `step_other`, and the full set of dummy variables to create. This is the step that prevents data leakage.\n\n2.  **`bake(prep, new_data)`:** After prepping, you can **`bake()`** the recipe. This applies the learned transformations to any dataset, such as the training data or, more importantly, new data like a test set.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prep the recipe using the training data\nprepped_recipe <- prep(tree_rec, training = trees_train)\n\n# Bake the training data to see the result of the transformations\n# Using new_data = NULL bakes the original training data\nprocessed_training_data <- bake(prepped_recipe, new_data = NULL)\n\n# Display the first few rows of the processed data\nhead(processed_training_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 11\n  tree_id height_m diameter_cm legal_status  date_planted_year species_Oak\n    <int>    <dbl>       <dbl> <fct>                     <dbl>       <dbl>\n1       1   -0.161     -0.108  Protected                -1.02        1.45 \n2       2    0.377      0.379  Not Protected            -0.305      -0.621\n3       3   -0.699     -0.595  Protected                -0.842      -0.621\n4       4    1.18       0.866  Protected                 0.592       1.45 \n5       5    0.646      0.671  Not Protected             0.233      -0.621\n6       6    0.108      0.0866 Protected                -1.02       -0.621\n# ℹ 5 more variables: species_Pine <dbl>, species_other_level <dbl>,\n#   caretaker_Private <dbl>, caretaker_other_level <dbl>,\n#   site_info_other_level <dbl>\n```\n\n\n:::\n:::\n\nNow, let's apply the same, already-prepped recipe to our `new_trees_data`.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bake the new data\nprocessed_new_data <- bake(prepped_recipe, new_data = new_trees_data)\n\n# Display the processed new data\nprocessed_new_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 11\n  tree_id height_m diameter_cm legal_status date_planted_year species_Oak\n    <int>    <dbl>       <dbl> <fct>                    <dbl>       <dbl>\n1      11    -1.51       -1.57 Protected                 1.49       1.45 \n2      12    -2.58       -2.84 Protected                 2.56      -0.621\n# ℹ 5 more variables: species_Pine <dbl>, species_other_level <dbl>,\n#   caretaker_Private <dbl>, caretaker_other_level <dbl>,\n#   site_info_other_level <dbl>\n```\n\n\n:::\n:::\n\nNotice how the recipe correctly handled the new, unseen levels (\"Redwood\", \"State\", etc.) in the test set based on the rules it learned from the training set. The new levels were converted to the \"other_level\" dummy variable, and the normalization was applied using the mean and standard deviation from the *training* data.\n\n# 5. Common Recipe Steps and Selectors\n\nThe `recipes` package has a rich set of steps. Some of the most common include:\n\n-   `step_impute_*()`: For handling missing data (e.g., `step_impute_mean`, `step_impute_median`, `step_impute_knn`).\n-   `step_log()`: For log-transforming numeric variables.\n-   `step_corr()`: For removing highly correlated predictors.\n-   `step_downsample()` / `step_upsample()`: For addressing class imbalance.\n-   `step_pca()`: For Principal Component Analysis.\n\n### Selector Functions\n\nYou can use powerful **selector functions** to choose which variables a step should apply to. This makes your recipes more robust and scalable.\n\n-   `all_predictors()`: Selects all predictor variables.\n-   `all_outcomes()`: Selects all outcome variables.\n-   `all_numeric()`, `all_nominal()`: Selects variables of a specific type.\n-   `all_numeric_predictors()`: Selects all numeric predictors.\n-   `starts_with()`, `ends_with()`, `contains()`: Selects variables based on their names.\n\n# 6. Reference\n\n-   [Tidymodels `recipes` Documentation](https://recipes.tidymodels.org/)\n-   [Available Recipe Steps](https://recipes.tidymodels.org/reference/index.html)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}